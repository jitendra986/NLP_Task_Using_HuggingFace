{"cells":[{"cell_type":"markdown","id":"29ef44f6-876d-4c92-bfa2-885dac873ad9","metadata":{"id":"29ef44f6-876d-4c92-bfa2-885dac873ad9"},"source":["# Lesson 10: Image Retrieval"]},{"cell_type":"code","source":["  !pip install transformers\n","  !pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJ0simLcmH1_","executionInfo":{"status":"ok","timestamp":1709893564706,"user_tz":-60,"elapsed":27864,"user":{"displayName":"jitendra tiwari","userId":"04882265798590373880"}},"outputId":"2f9814a9-b174-47ca-8ea4-96285ecb06f2"},"id":"tJ0simLcmH1_","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"markdown","id":"fa61c20e","metadata":{"id":"fa61c20e"},"source":["- In the classroom, the libraries are already installed for you.\n","- If you would like to run this code on your own machine, you can install the following:\n","\n","```\n","    !pip install transformers\n","    !pip install torch\n","```"]},{"cell_type":"markdown","id":"190858c7-792d-4695-ba6d-b502c1d25b03","metadata":{"id":"190858c7-792d-4695-ba6d-b502c1d25b03"},"source":["- Here is some code that suppresses warning messages."]},{"cell_type":"code","execution_count":3,"id":"f1faec1a-dcda-4b44-84b8-1631f0bb464e","metadata":{"height":47,"id":"f1faec1a-dcda-4b44-84b8-1631f0bb464e","executionInfo":{"status":"ok","timestamp":1709893588573,"user_tz":-60,"elapsed":11827,"user":{"displayName":"jitendra tiwari","userId":"04882265798590373880"}}},"outputs":[],"source":["from transformers.utils import logging\n","logging.set_verbosity_error()"]},{"cell_type":"markdown","id":"847fac54","metadata":{"id":"847fac54"},"source":["- Load the model and the processor"]},{"cell_type":"code","execution_count":4,"id":"67da6b12-64f8-44b1-afd0-717e41346c84","metadata":{"height":30,"id":"67da6b12-64f8-44b1-afd0-717e41346c84","executionInfo":{"status":"ok","timestamp":1709893589676,"user_tz":-60,"elapsed":1110,"user":{"displayName":"jitendra tiwari","userId":"04882265798590373880"}}},"outputs":[],"source":["from transformers import BlipForImageTextRetrieval"]},{"cell_type":"code","execution_count":5,"id":"417991ee-dbf7-48fe-83a6-b6d5700632be","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/","height":463},"id":"417991ee-dbf7-48fe-83a6-b6d5700632be","executionInfo":{"status":"error","timestamp":1709893612678,"user_tz":-60,"elapsed":616,"user":{"displayName":"jitendra tiwari","userId":"04882265798590373880"}},"outputId":"3355fbb3-8428-4a37-c0f6-72eba3a50f4b"},"outputs":[{"output_type":"error","ename":"OSError","evalue":"Incorrect path_or_model_id: './models/Salesforce/blip-itm-base-coco'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/Salesforce/blip-itm-base-coco'. Use `repo_type` argument if needed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-66d8ed03677f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = BlipForImageTextRetrieval.from_pretrained(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"./models/Salesforce/blip-itm-base-coco\")\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2872\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2873\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2874\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   2875\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2876\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         ) from e\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './models/Salesforce/blip-itm-base-coco'. Please provide either the path to a local folder or the repo_id of a model on the Hub."]}],"source":["model = BlipForImageTextRetrieval.from_pretrained(\n","    \"./models/Salesforce/blip-itm-base-coco\")"]},{"cell_type":"markdown","id":"f80ab891","metadata":{"id":"f80ab891"},"source":["More info about [Salesforce/blip-itm-base-coco](https://huggingface.co/Salesforce/blip-itm-base-coco)."]},{"cell_type":"code","execution_count":null,"id":"478a6f97-a290-42cd-9f2c-284ea7e41d49","metadata":{"height":30,"id":"478a6f97-a290-42cd-9f2c-284ea7e41d49"},"outputs":[],"source":["from transformers import AutoProcessor"]},{"cell_type":"code","execution_count":null,"id":"50faf354-91e4-4739-8e2d-7547504e4c92","metadata":{"height":47,"id":"50faf354-91e4-4739-8e2d-7547504e4c92"},"outputs":[],"source":["processor = AutoProcessor.from_pretrained(\n","    \"./models/Salesforce/blip-itm-base-coco\")"]},{"cell_type":"code","execution_count":null,"id":"7e72f24c-86b8-48d3-af80-07434ea43522","metadata":{"height":30,"id":"7e72f24c-86b8-48d3-af80-07434ea43522"},"outputs":[],"source":["img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'"]},{"cell_type":"code","execution_count":null,"id":"9e17973f-f89a-40a4-9e4a-b46f46d4dfba","metadata":{"height":47,"id":"9e17973f-f89a-40a4-9e4a-b46f46d4dfba"},"outputs":[],"source":["from PIL import Image\n","import requests"]},{"cell_type":"code","execution_count":null,"id":"7d711df0-1f01-42e5-b397-8ffcb6ef4c12","metadata":{"height":47,"id":"7d711df0-1f01-42e5-b397-8ffcb6ef4c12"},"outputs":[],"source":["raw_image =  Image.open(\n","    requests.get(img_url, stream=True).raw).convert('RGB')"]},{"cell_type":"code","execution_count":null,"id":"cc0a6f43-1607-4364-a7d1-c9799c1a19af","metadata":{"height":30,"id":"cc0a6f43-1607-4364-a7d1-c9799c1a19af"},"outputs":[],"source":["raw_image"]},{"cell_type":"markdown","id":"38db1166-f8c1-4f06-ad67-7083492b6ceb","metadata":{"id":"38db1166-f8c1-4f06-ad67-7083492b6ceb"},"source":["### Test, if the image matches the text"]},{"cell_type":"code","execution_count":null,"id":"1edee591-c891-4b4b-91ee-1e9f54642e97","metadata":{"height":30,"id":"1edee591-c891-4b4b-91ee-1e9f54642e97"},"outputs":[],"source":["text = \"an image of a woman and a dog on the beach\""]},{"cell_type":"code","execution_count":null,"id":"04b41ec7-70d7-49c2-909a-e600ab7e76ca","metadata":{"height":64,"id":"04b41ec7-70d7-49c2-909a-e600ab7e76ca"},"outputs":[],"source":["inputs = processor(images=raw_image,\n","                   text=text,\n","                   return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":null,"id":"5f818f97-3b9a-4024-8cab-31b615d32632","metadata":{"height":30,"id":"5f818f97-3b9a-4024-8cab-31b615d32632"},"outputs":[],"source":["inputs"]},{"cell_type":"code","execution_count":null,"id":"db51ff2e-74c2-4ea1-b271-48d8f15e7fd6","metadata":{"height":30,"id":"db51ff2e-74c2-4ea1-b271-48d8f15e7fd6"},"outputs":[],"source":["itm_scores = model(**inputs)[0]"]},{"cell_type":"code","execution_count":null,"id":"c962d20d-6091-4bba-9cc9-afa9a3bef79b","metadata":{"height":30,"id":"c962d20d-6091-4bba-9cc9-afa9a3bef79b"},"outputs":[],"source":["itm_scores"]},{"cell_type":"code","execution_count":null,"id":"7615e88d-a6c1-4363-bcab-0f11e93d20af","metadata":{"height":30,"id":"7615e88d-a6c1-4363-bcab-0f11e93d20af"},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","id":"bc0a433f-d1f1-45c7-b776-1472b8a7cbcd","metadata":{"id":"bc0a433f-d1f1-45c7-b776-1472b8a7cbcd"},"source":["- Use a softmax layer to get the probabilities"]},{"cell_type":"code","execution_count":null,"id":"b0749cbf-be41-4b47-92ac-a0f6515ff31e","metadata":{"height":47,"id":"b0749cbf-be41-4b47-92ac-a0f6515ff31e"},"outputs":[],"source":["itm_score = torch.nn.functional.softmax(\n","    itm_scores,dim=1)"]},{"cell_type":"code","execution_count":null,"id":"adc3f239-f95e-4451-9a5b-b8b83011eeb4","metadata":{"height":30,"id":"adc3f239-f95e-4451-9a5b-b8b83011eeb4"},"outputs":[],"source":["itm_score"]},{"cell_type":"code","execution_count":null,"id":"1f000eb6-2b6a-48c5-8481-a3bbdd5b3c5d","metadata":{"height":64,"id":"1f000eb6-2b6a-48c5-8481-a3bbdd5b3c5d"},"outputs":[],"source":["print(f\"\"\"\\\n","The image and text are matched \\\n","with a probability of {itm_score[0][1]:.4f}\"\"\")"]},{"cell_type":"markdown","id":"416bc237","metadata":{"id":"416bc237"},"source":["### Try it yourself!\n","- Try this model with your own images and texts!"]},{"cell_type":"code","execution_count":null,"id":"02012ee6-68f8-4b18-ba64-9596cdcd9f9d","metadata":{"height":30,"id":"02012ee6-68f8-4b18-ba64-9596cdcd9f9d"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}